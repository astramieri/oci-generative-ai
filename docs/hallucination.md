# Hallucination

LLM-generated text is somewhat unpredictable. It's often good, fluent, and accurate, but sometimes, it's not factual or even unsafe.

**Hallucination** is text generated by a model that is not grounded by any data the model has been exposed to. In other word, it is non-factual generated text.

Hallucination is especially problematic and dangerous when the model is generating text about a topic that the consumer does not know much about and cannot verify the veracity of easily.

The threat of hallucination is one of the biggest challenges to safely deploying LLMs. 

    "Think about LLMs as chameleons. They're trying to generate text that blends in with human generated text, whether or not it's true. It just needs to sound true." Samir Singh

**There is no known method that will eliminate hallucination with 100% certainty**. On the other hand, there is a growing set of best practices and typical precautions to take when using LLMs to generate text. As one example, there is some evidence that shows that *retrieval-augmented systems* hallucinate less than zero-shot LLMs.

## Groundedness and Attributability

In a related line of work that is growing in popularity, researchers are developing methods for measuring the **groundedness** of LLM-generated output.

These methods work by taking a sentence generated by an LLM and a candidate's supporting document and outputting whether the document supports the output sentence or not. These methods work by training a separate model to perform *Natural Language Inference (NLI)*, which is the task that's been studied in the NLP community for a long time. In this task, you're given a premise that is some text and a hypothesis, maybe a generated sentence. The goal is to predict whether the premise entails the hypothesis. 

At the same time, new grounded versions of question answering have been proposed, which is the task of answering questions while also citing the sources of the answer being provided. And more work has focused on citation and attribution of LLM-generated content.